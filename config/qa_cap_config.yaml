Task:
  name: 'video_cap'
  tag: ''
  result_dir: './result/'
  visual_reduce: True
  semantic_reduce: False
    # object_det: /mnt/cephfs/dataset/activitynet_captions/object_det
    # action_rec: /mnt/cephfs/dataset/activitynet_captions/action_cls
    # image_cap: /mnt/cephfs/dataset/activitynet_captions/caption
    # videochat: ./result/videochat_cap
    # clip_cap: /mnt/cephfs/dataset/activitynet_captions/clip_cap
    # video_cap
    # clip_caption_videochat
    # atomic_clipcap_videochat
Data:
  set_name: 'ego'
  fps: 16
  clip_length: 8
  clip_stride: 8
  detect_fps: 16
  data_paths:
    anet:
      file: './data/captions/val_1.json'
      video_dir: /youzeng/datasets/anet_videos/ #/mnt/cephfs/dataset/activitynet_video/all_videos/
    msrvtt:
      file: './data/qa/msrvtt/test_qa_re.json'
      video_dir: '/youzeng/datasets/msrvtt/MSRVTT/videos/all'
    youcook:
      file: './data/youcook2/youcookii_annotations_trainval.json'
      video_dir: /youzeng/datasets/youcook2/
    didemo:
      file: './data/DiDeMo/test_data_aval.json'
      video_dir: /youzeng/datasets/didemo/test_videos
    ego:
      file: './data/ego_schema/ego_schema.json'
      video_dir: '/youzeng/datasets/EgoSchema/good_clips_git/'
Model:
  AtomicAgents:  ['vicuna'] # optional: ['object_det','action_rec','image_cap','videochat','vicuna', 'text_encoder','clip', 'angle']
  ObjectDetector:
    config: ../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py
    checkpoint_path: /youzeng/pretrained_weights/groudingdino/groundingdino_swint_ogc.pth
    cls_file: ["./data/cls_labels/coco_cls.txt","./data/cls_labels/o365.txt", "./data/cls_labels/extra_cls.txt"]
    exclude_cls: "./data/cls_labels/exclude_cls.txt"
    box_threshold: 0.4
    text_threshold: 0.25
  CLIP:
    model_path: /youzeng/pretrained_weights/clip-vit-base-patch32
  ActionRecognizer:
    model_path: /youzeng/pretrained_weights/internvideo/k400+k710_uniformerv2_b16_8x224.pyth
  SentenceEncoder:
    model_path: '/youzeng/pretrained_weights/bert-base-nli-mean-tokens'
  VideoChat:
    config: ./config/config.json
    generation:
      max_new_tokens: 320
      num_beams: 1
      min_length: 5
      top_p: 0.9
      repetition_penalty: 1.0
      length_penalty: 1
      temperature: 1.0
  ImgCaption:
    prompt: 'Describe this picture in as much detail as possible, including where this picture is located, what objects are there and what color they are.'
    model_path: /youzeng/pretrained_weights/blip2-flan-t5-xl/
    max_length: 256
    min_length: 50
    repetition_penalty: 3.0
  Vicuna:
    clip_prompt: "A chat between a user who provides information about the content in a video and an assistant who can summarize the content of the video based on the information provided by the user."
    video_prompt: "A chat between a user who provides information about the content in a video and an assistant who can describe the content of the video in detail using 'first', 'then', 'after that',  and 'finnally' based on the information provided by the user. User: "
    model_path: '/youzeng/pretrained_weights/vicuna-13b-v1.5-16k'
    max_new_tokens: 100
    temperature: 0.7
    repetition_penalty: 1.0
    min_length: 5
    length_penalty: 1.0
    bs: 1
  Angle:
    pretrianed_weights: /youzeng/pretrained_weights/Llama-2-13b-hf
    model_path: /youzeng/pretrained_weights/angle-llama-13b-nli
Device: cuda
Infer:
  distributed: True
  verbose: ./result/
Template:
  prompt: "video information: <action_prompt> <object_prompt>"
  object: "in <detect_time>s: there are <object_info>. "
  action: "the action recognized in this clip is \"<action_cls>\". "
  caption: "I use object detection model and action recognition model identify some content from this video. the information is as follows: <video_info> \
  Please describe the objects and what happened in the video in detail based on the information I give you and the video. The information \
  I give may not be correct, so you need to correct it for the video."
