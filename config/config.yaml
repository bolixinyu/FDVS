Task:
  name: 'video_cap'
  tag: '_frame32'
  result_dir: './result/'
  visual_reduce: True
  semantic_reduce: False
    # object_det: /mnt/cephfs/dataset/activitynet_captions/object_det
    # action_rec: /mnt/cephfs/dataset/activitynet_captions/action_cls
    # image_cap: /mnt/cephfs/dataset/activitynet_captions/caption
    # videochat: ./result/videochat_cap
    # clip_cap: /mnt/cephfs/dataset/activitynet_captions/clip_cap
    # video_cap
    # clip_caption_videochat
    # atomic_clipcap_videochat
    # clip_cap_llama2
    # video_cap_llama
Data:
  set_name: 'msrvtt'
  fps: 16
  clip_length: 16
  clip_stride: 8
  detect_fps: 16
  data_paths:
    anet:
      file: './data/captions/val_1.json'
      video_dir: /path/to/anet_videos/ #/mnt/cephfs/dataset/activitynet_video/all_videos/
    msrvtt:
      file: './data/msrvtt/test.json'
      video_dir: '/path/to/msrvtt/all_videos'
    youcook:
      file: './data/youcook2/youcookii_annotations_trainval.json'
      video_dir: /path/to/youcook2/
    didemo:
      file: './data/DiDeMo/test_data_aval.json'
      video_dir: /path/to/didemo/test_videos
    charades:
      file: './data/charades/charades_test.json'
      video_dir: '/path/to/Charades_v1_480'
    ego:
      file: './data/ego_schema/ego_schema.json'
      video_dir: '/path/to/EgoSchema/'
    next:
      file: './data/next_qa/val_map.json'
      video_dir: '/path/to/NextVideo/videos'
Model:
  AtomicAgents:  ['vicuna'] # optional: ['object_det','action_rec','image_cap','videochat','vicuna', 'text_encoder','clip', 'angle', 'llama', 'lavila]
  ObjectDetector:
    config: /path/to/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py
    checkpoint_path: /path/to/groundingdino/groundingdino_swinb_cogcoor.pth
    cls_file: ["./data/cls_labels/coco_cls.txt","./data/cls_labels/o365.txt", "./data/cls_labels/extra_cls.txt"]
    exclude_cls: "./data/cls_labels/exclude_cls.txt"
    box_threshold: 0.4
    text_threshold: 0.25
  CLIP:
    model_path: /path/to/clip-vit-base-patch32/
  ActionRecognizer:
    model_path: /path/to/intern_video//k400+k710_uniformerv2_b16_8x224.pyth
  SentenceEncoder:
    model_path: '/path/to/bert-base-nli-mean-tokens'
  VideoChat:
    config: ./config/config_smil.json
    generation:
      max_new_tokens: 320
      num_beams: 1
      min_length: 5
      top_p: 0.9
      repetition_penalty: 1.0
      length_penalty: 1
      temperature: 1.0
  ImgCaption:
    # prompt: 'Describe this picture in as much detail as possible, including where this picture is located, what objects are there and what color they are.'
    prompt: 'Describe this picture in detail, including what happend, what objects are there, and what color they are.'
    model_path: /path/to/blip2-flan-t5-xl/
    max_length: 256
    min_length: 30
    repetition_penalty: 1.0
  Vicuna:
    # clip_prompt: "A chat between a user who provides information about the content in a video and an assistant who can summarize the content of the video based on the information provided by the user."
    clip_prompt: "A chat between a user who provides information about the content in a video and an assistant who can summarize the content of the each video based on the information provided by the user. Each video should be summarized in 100 words."
    video_prompt: "A chat between a user who provides information about the content in a video and an assistant who can describe the content of the video in detail using 'first', 'then', 'after that',  and 'finnally' based on the information provided by the user. User: "
    model_path: '/path/to/vicuna-7b-v1.5-16k'
    max_new_tokens: 100
    temperature: 0.7
    repetition_penalty: 1.0
    min_length: 5
    length_penalty: 1.0
    bs: 1
  Angle:
    pretrianed_weights: /path/to/Llama-2-13b-hf
    model_path: /path/to/angle-llama-7b-nli
  LlaMa2:
    model_path: /path/to/Llama-2-7b-chat-hf/
    max_new_tokens: 100
    temperature: 0
    repetition_penalty: 1.0
    min_length: 5
    length_penalty: 1.0
    bs: 1
  Lavila:
    model_path: /path/to/lavila/vclm_openai_timesformer_large_336px_gpt2_xl.pt_ego4d.jobid_246897.ep_0003.md5sum_443263.pth
    clip_path: /path/to/lavila/ViT-L-14-336px.pt
    gpt2_xl_path: /path/to/gpt2-xl

Device: cuda
Infer:
  distributed: True
  verbose: ./result/
Template:
  prompt: "video information: <action_prompt> <object_prompt>"
  object: "in <detect_time>s: there are <object_info>. "
  action: "the action recognized in this clip is \"<action_cls>\". "
  caption: "I use object detection model and action recognition model identify some content from this video. the information is as follows: <video_info> \
  Please describe the objects and what happened in the video in detail based on the information I give you and the video. The information \
  I give may not be correct, so you need to correct it for the video."
