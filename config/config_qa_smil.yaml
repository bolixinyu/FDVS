Task:
  tag: 'llama_mc'
  result_dir: './result/qa'
    # object_det: /mnt/cephfs/dataset/activitynet_captions/object_det
    # action_rec: /mnt/cephfs/dataset/activitynet_captions/action_cls
    # image_cap: /mnt/cephfs/dataset/activitynet_captions/caption
    # videochat: ./result/videochat_cap
    # clip_cap: /mnt/cephfs/dataset/activitynet_captions/clip_cap
    # video_cap
    # clip_caption_videochat
    # atomic_clipcap_videochat
Data:
  set_name: 'ego_qa'
  data_paths:
    anet_qa:
      file: './data/qa/anet/val_qa_pair.json'
      video_dir: /path/to/anet_videos/ #/mnt/cephfs/dataset/activitynet_video/all_videos/
      clip_cap: './result/anet/clip_cap_visual_reduce_all/'
      text_emb: './m_result/anet/'
      video_cap: './result/anet/video_cap_visual_semantic_reduce_all/'
    msrvtt_qa:
      file: './data/qa/msrvtt/test_qa_re.json'
      video_dir: '/youzeng/datasets/msrvtt/MSRVTT/videos/all'
      clip_cap: './result/msrvtt/clip_cap_interpretable_all_visual_reduce'
      text_emb: './m_result/msrvtt/'
      video_cap: './result/msrvtt/video_cap_interpretable_all_visual_reduce'
    youcook:
      file: './data/youcook2/youcookii_annotations_trainval.json'
      video_dir: /path/to/youcook2/
    didemo:
      file: './data/DiDeMo/test_data_aval.json'
      video_dir: /path/to/didemo/test_videos
    ego_qa:
      file: './data/ego_schema/ego_schema.json'
      video_dir: /mnt/cephfs/dataset/EgoSchema/good_clips_git/ #/mnt/cephfs/dataset/activitynet_video/all_videos/
      clip_cap: './result/rebuttal_exp/ego/clip_cap_llama2/'
      text_emb: './m_result/ego_llama/'
      video_cap: './result/ego/video_cap/'
Model:
  models:  [llama2] # optional: ['sentence', 'angle','llama2']
  bs: 1
  SentenceEncoder:
    model_path: '/mnt/cephfs/dataset/huggingface_models/pretrained_weights//bert-base-nli-mean-tokens'
  Vicuna:
    clip_prompt: "A chat between a user who provides information about the content in a video and an assistant who can summarize the content of the video based on the information provided by the user."
    video_prompt: "A chat between a user who provides information about the content in a video and an assistant who can describe the content of the video in detail using 'first', 'then', 'after that',  and 'finnally' based on the information provided by the user. User: "
    model_path: '/mnt/cephfs/dataset/huggingface_models/pretrained_weights/vicuna-7b-v1.5-16k'
    max_new_tokens: 100
    temperature: 0.7
    repetition_penalty: 1.0
    min_length: 5
    length_penalty: 1.0
  Angle:
    pretrianed_weights: '/userhome/pretrained_weights/Llama-2-13b-hf'
    model_path: '/userhome/pretrained_weights/angle-llama-13b-nli'
  LlaMa2:
    model_path: '/userhome/pretrained_weights/Llama-2-13b-chat-hf'
Device: cuda
Infer:
  distributed: False
  verbose: ./result/
  topk: 5
